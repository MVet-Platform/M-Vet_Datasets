{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e30819-1bc7-462e-88e2-64b555523df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539dd9b-6162-4c0a-96cd-e91974d5e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Computes Intersection over Union (IoU) between two bounding boxes.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1g, y1g, x2g, y2g = box2\n",
    "\n",
    "    # Compute the coordinates of the intersection rectangle\n",
    "    xi1 = max(x1, x1g)\n",
    "    yi1 = max(y1, y1g)\n",
    "    xi2 = min(x2, x2g)\n",
    "    yi2 = min(y2, y2g)\n",
    "\n",
    "    # Compute the area of intersection\n",
    "    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
    "\n",
    "    # Compute the area of both bounding boxes\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
    "\n",
    "    # Compute the union area\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = inter_area / union_area if union_area > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e01b3-f19c-474c-9711-cda767af4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap50(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes Average Precision at IoU = 0.5 (AP@50)\n",
    "    \"\"\"\n",
    "    # Sort predicted boxes by confidence score\n",
    "    pred_boxes = sorted(pred_boxes, key=lambda x: x[4], reverse=True)\n",
    "\n",
    "    # Initialize True Positive (TP) and False Positive (FP) arrays\n",
    "    tp = np.zeros(len(pred_boxes))\n",
    "    fp = np.zeros(len(pred_boxes))\n",
    "\n",
    "    # Track which ground truth boxes have been used\n",
    "    detected_gt = []\n",
    "\n",
    "    for i, pred_box in enumerate(pred_boxes):\n",
    "        iou_max = 0\n",
    "        matched_gt_idx = -1\n",
    "        \n",
    "        for j, gt_box in enumerate(gt_boxes):\n",
    "            iou = compute_iou(pred_box[:4], gt_box[:4])\n",
    "            if iou > iou_max:\n",
    "                iou_max = iou\n",
    "                matched_gt_idx = j\n",
    "\n",
    "        # If IoU is greater than threshold and the GT box has not been detected\n",
    "        if iou_max >= iou_threshold and matched_gt_idx not in detected_gt:\n",
    "            tp[i] = 1\n",
    "            detected_gt.append(matched_gt_idx)\n",
    "        else:\n",
    "            fp[i] = 1\n",
    "    # Compute precision and recall\n",
    "    cum_tp = np.cumsum(tp)\n",
    "    cum_fp = np.cumsum(fp)\n",
    "    precision = cum_tp / (cum_tp + cum_fp)\n",
    "    recall = cum_tp / len(gt_boxes)\n",
    "\n",
    "    # Compute Average Precision (AP) as the area under the precision-recall curve\n",
    "    ap = np.sum(precision * np.diff(np.hstack(([0], recall))))\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd2bc6-4f6c-4934-abc6-62affd873f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_map50(all_predictions, all_ground_truths):\n",
    "    \"\"\"\n",
    "    Computes the mean Average Precision at IoU=0.5 (mAP@50) for a specific class.\n",
    "    \"\"\"\n",
    "    ap50s = []\n",
    "    for pred_boxes, gt_boxes in zip(all_predictions, all_ground_truths):\n",
    "        ap50 = compute_ap50(pred_boxes, gt_boxes)\n",
    "        ap50s.append(ap50)\n",
    "    map50 = np.mean(ap50s)\n",
    "    return map50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea974f-1abe-4f58-a66a-891de6a7451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_boxes(df, is_submission=True):\n",
    "    if is_submission:\n",
    "        return df[['xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class']].values.tolist()\n",
    "    else:\n",
    "        return df[['xmin', 'ymin', 'xmax', 'ymax', 'class']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c857ed6-ca49-4296-b3bb-03f46d84bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_map50(gt_df, submission_df):\n",
    "    \"\"\"\n",
    "    Evaluates mAP@50 given ground truth and submission DataFrames.\n",
    "    \"\"\"\n",
    "    # Group by filename or image_id\n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    \n",
    "    for filename in tqdm(gt_df['filename'].unique(), desc='Getting prediction and ground truth boxes'):\n",
    "        gt_boxes = df_to_boxes(gt_df[gt_df['filename'] == filename], is_submission=False)\n",
    "        pred_boxes = df_to_boxes(submission_df[submission_df['filename'] == filename], is_submission=True)\n",
    "        \n",
    "        all_ground_truths.append(gt_boxes)\n",
    "        all_predictions.append(pred_boxes)\n",
    "    \n",
    "    # Compute mAP@50\n",
    "    map50 = compute_map50(all_predictions, all_ground_truths)\n",
    "    return map50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf110b4-a332-43de-aa53-127b951121b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(gt_file_path, submission_file_path):\n",
    "    gt = pd.read_csv(gt_file_path)\n",
    "    submission = pd.read_csv(submission_file_path)\n",
    "    classes = ['cow', 'goat', 'pig']\n",
    "    total_map = 0\n",
    "    for cls in classes:\n",
    "        gt_cls_df = gt[gt['class']==cls]\n",
    "        submission_cls_df = submission[submission['class']==cls].copy()\n",
    "        map = evaluate_map50(gt_cls_df, submission_cls_df)\n",
    "        total_map+=map\n",
    "        print(f'class: {cls}\\tmap: {map}')\n",
    "    print(f'Average Map: {total_map/3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
